import json
from typing import Any

from openai import OpenAI
from dotenv import load_dotenv
import os

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

class OpenAiClient:
    def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = 30):
        self.model = model
        self.client = OpenAI(api_key=apiKey, base_url=baseUrl, timeout=timeout)
    def chat(self, messages: list[dict[str, str]], temperature: int = 0, stream: bool = False):
        """
         è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›å…¶å“åº”ã€‚
        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        try:
            response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            stream=stream)
            print(f"ResponseType: {type(response)}")
            print(f"Response: {response}")
            print(f"ResponseJson: {response.model_dump_json()}")
        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return response

class ChainOfThought:
    def __init__(self, client: OpenAiClient):
        self.client = client
    def solve_step_by_step(self, user_input: str) -> dict[str, Any]:
        # ç¬¬ä¸€æ­¥åˆ†è§£é—®é¢˜
        decomposition_prompt = f"""
            è¯·å°†é—®é¢˜åˆ†è§£ä¸º3-5ä¸ªé€»è¾‘æ­¥éª¤:
            é—®é¢˜: {user_input}
            
            è¯·è¿”å›JSONæ ¼å¼:
            {{
                "steps":[
                    {{"step": 1, "description": "æ­¥éª¤æè¿°", "question": "éœ€è¦å›ç­”çš„å…·ä½“é—®é¢˜"}},
                ...
                ]
            }}
        """
        messages = [{"role": "user", "content": decomposition_prompt}]
        response = self.client.chat(messages)
        try:
            steps_data = json.loads(response.choices[0].message.content)
            steps = steps_data.get("steps", [])
        except json.JSONDecodeError:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•åˆ†è§£
            steps = [
                {"step": 1, "description": "ç†è§£é—®é¢˜", "question": f"å¦‚ä½•ç†è§£è¿™ä¸ªé—®é¢˜ï¼š{user_input}ï¼Ÿ"},
                {"step": 2, "description": "åˆ†æè§£å†³æ–¹æ¡ˆ", "question": "æœ‰å“ªäº›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Ÿ"},
                {"step": 3, "description": "å¾—å‡ºç»“è®º", "question": "æœ€ä½³ç­”æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿ"}
            ]
        print(f"ğŸ”— é—®é¢˜å·²åˆ†è§£ä¸º {len(steps)} ä¸ªæ­¥éª¤")

        # é€æ­¥è§£å†³é—®é¢˜
        step_results = []
        context = f"åŸå§‹é—®é¢˜ï¼š{user_input}\n\n"
        for step in steps:
            step_num = step["step"]
            description = step["description"]
            question = step["question"]
            print(f"  ğŸ“ æ­¥éª¤ {step_num}: {description}")
            step_prompt = f"""
                        {context}
                        å½“å‰æ­¥éª¤ï¼š{description}
                        å…·ä½“é—®é¢˜ï¼š{question}

                        è¯·åŸºäºå‰é¢çš„åˆ†æï¼Œè¯¦ç»†å›ç­”è¿™ä¸ªæ­¥éª¤çš„é—®é¢˜ã€‚
                        """
            step_message = [{"role": "user", "content": step_prompt}]
            step_response = self.client.chat(step_message)
            step_result = step_response.choices[0].message.content
            step_results.append({
                "step": step_num,
                "description": description,
                "question": question,
                "answer": step_result
            })
            # æ›´æ–°ä¸Šä¸‹æ–‡
            context += f"æ­¥éª¤{step_num} ({description}): {step_result}\n\n"

        # æœ€ç»ˆç»¼åˆ
        final_prompt = f"""
            åŸºäºä»¥ä¸‹é€æ­¥åˆ†æï¼Œè¯·ç»™å‡ºå¯¹åŸå§‹é—®é¢˜çš„æœ€ç»ˆç»¼åˆç­”æ¡ˆï¼š

            {context}

            è¯·æä¾›ä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„æœ€ç»ˆç­”æ¡ˆã€‚
            """
        final_response = self.client.chat(messages=[{"role": "user", "content": final_prompt}])
        final_answer = final_response.choices[0].message.content
        return {
            "problem": user_input,
            "steps": step_results,
            "final_answer": final_answer,
            "total_steps": len(steps)
        }


if __name__ == '__main__':
    model = os.getenv("LLM_MODEL_ID")
    apiKey = os.getenv("LLM_API_KEY")
    baseUrl = os.getenv("LLM_BASE_URL")
    messages = [
        {"role": "user", "content": "ä¸€ä¸ªç­çº§æœ‰30ä¸ªå­¦ç”Ÿï¼Œå…¶ä¸­60%æ˜¯å¥³ç”Ÿã€‚å¦‚æœæ–°æ¥äº†5ä¸ªç”·ç”Ÿï¼Œç°åœ¨ç”·ç”Ÿå’Œå¥³ç”Ÿçš„æ¯”ä¾‹æ˜¯å¤šå°‘ï¼Ÿ"}
    ]
    llm_client = OpenAiClient(model=model, apiKey=apiKey, baseUrl=baseUrl)
    llm_client.chat(messages=messages)

    math_problem = "ä¸€ä¸ªç­çº§æœ‰30ä¸ªå­¦ç”Ÿï¼Œå…¶ä¸­60%æ˜¯å¥³ç”Ÿã€‚å¦‚æœæ–°æ¥äº†5ä¸ªç”·ç”Ÿï¼Œç°åœ¨ç”·ç”Ÿå’Œå¥³ç”Ÿçš„æ¯”ä¾‹æ˜¯å¤šå°‘ï¼Ÿ"
    cot = ChainOfThought(client=llm_client)
    cot_result = cot.solve_step_by_step(math_problem)
    print(f"ğŸ“ é—®é¢˜: {cot_result['problem']}")
    print(f"ğŸ”¢ åˆ†è§£æ­¥éª¤æ•°: {cot_result['total_steps']}")

    print("\nğŸ“‹ è¯¦ç»†æ­¥éª¤:")
    for step in cot_result['steps']:
        print(f"  {step['step']}. {step['description']}")
        print(f"     é—®é¢˜: {step['question']}")
        print(f"     ç­”æ¡ˆ: {step['answer'][:100]}...")
        print()

    print(f"ğŸ¯ æœ€ç»ˆç­”æ¡ˆ: {cot_result['final_answer'][:200]}...")

    print("\n" + "=" * 70 + "\n")
