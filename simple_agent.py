import json
import os
import time
from dataclasses import field, dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Optional

import requests
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

class LlmClient:
        def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = 600, stream: bool = False):
            self.model = model
            self.apiKey = apiKey
            self.baseUrl = baseUrl
            self.timeout = timeout
            self.stream = stream
            self.headers: dict[str, str] = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {self.apiKey}'
            }
        def chat(self, messages: list[dict[str, str]], temperature: int = 0):
            data = {
                'model': self.model,
                'messages': messages,
                'temperature': temperature,
            }
            url = f'{self.baseUrl}/chat/completions'
            response = requests.post(url, json=data, headers=self.headers, timeout=self.timeout)
            return response

class OpenAiClient:
    def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = 30):
        self.model = model
        self.client = OpenAI(api_key=apiKey, base_url=baseUrl, timeout=timeout)
    def chat(self, messages: list[dict[str, str]], temperature: int = 0, stream: bool = False):
        """
         è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›å…¶å“åº”ã€‚
        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        try:
            response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            stream=stream)
            print(f"ResponseType: {type(response)}")
            print(f"Response: {response}")
            print(f"ResponseJson: {response.model_dump_json()}")
        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")

class AgentState(Enum):
    """AgentçŠ¶æ€æšä¸¾"""
    IDLE = "idle" # ç©ºé—²
    PERCEIVING = "perceiving" # æ„ŸçŸ¥
    PLANNING = "planning" # è§„åˆ’
    ACTING = "acting" # æ‰§è¡Œ
    REFLECTING = "reflecting" # åæ€
    ERROR = "error" # é”™è¯¯

@dataclass
class AgentMemory:
    """Agent è®°å¿†æ¨¡å—"""
    short_memory: list[dict[str, Any]] = field(default_factory=list)
    long_memory: dict[str, Any] = field(default_factory=dict)
    working_memory: dict[str, Any] = field(default_factory=dict)

    def add_short_memory(self, memory: dict[str, Any]):
        memory["timestamp"] = datetime.now().isoformat()
        self.short_memory.append(memory)
        if len(self.short_memory) > 10:
            # é™åˆ¶çŸ­æœŸè®°å¿†å¤§å°
            self.short_memory.pop(0)

    def update_working_memory(self, key: str, value: Any):
        self.working_memory[key] = value

    def get_context(self) -> str:
        context_parts = []
        if self.working_memory:
            context_parts.append(f"å½“å‰çŠ¶æ€:{json.dumps(self.working_memory, ensure_ascii=False, indent=2)}")
        if self.short_memory:
            recent_memories = self.short_memory[-3:] # æœ€è¿‘çš„ä¸‰æ¡è®°å¿†
            memory_context = "\n".join([f"- {mem.get('content',mem)}"for mem in recent_memories])
            context_parts.append(memory_context)
        return "\n\n".join(context_parts)

@dataclass
class Task:
    id: str
    description: str
    priority: int = 1
    status: str = "pending"
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())
    completed_at: Optional[str] = None
    result: Optional[str] = None
    error: Optional[str] = None

class SimpleAgent:
    def __init__(self, name:str, role: str = "é€šç”¨åŠ©æ‰‹", llm: LlmClient = None):
        self.name = name
        self.role = role
        self.state = AgentState.IDLE
        self.memory: AgentMemory = AgentMemory()
        self.tasks: list[Task] = []
        self.tools = {} # å·¥å…·
        self.LlmClient = llm
        self.system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªåä¸º{name}çš„AI Agentï¼Œè§’è‰²æ˜¯{role}ã€‚
        ä½ çš„èƒ½åŠ›åŒ…æ‹¬ï¼š
        1. ç†è§£å’Œåˆ†æç”¨æˆ·éœ€æ±‚
        2. åˆ¶å®šæ‰§è¡Œè®¡åˆ’
        3. æ‰§è¡Œå…·ä½“ä»»åŠ¡
        4. åæ€å’Œæ€»ç»“ç»éªŒ
        
        ä½ å¿…é¡»å§‹ç»ˆä¿æŒï¼š
        - é€»è¾‘æ¸…æ™°çš„æ€è€ƒè¿‡ç¨‹
        - è¯¦ç»†çš„æ­¥éª¤è¯´æ˜
        - å‹å¥½å’Œä¸“ä¸šçš„äº¤æµæ–¹å¼
        """.strip()
        print(f"ğŸ¤– Agent '{name}' åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“‹ è§’è‰²: {role}")
        print(f"ğŸ”„ çŠ¶æ€: {self.state.value}")

    def call_llm(self, messages: list[dict[str, str]], temperature: int = 0) -> str:
        try:
            response = self.LlmClient.chat(messages=messages, temperature=temperature)
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {response.status_code}")
        except Exception as e:
            return f"æŠ±æ­‰ï¼Œåœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜:{str(e)}"

    def perceive(self,input_data: str) -> dict[str, Any]:
        """æ„ŸçŸ¥é˜¶æ®µï¼šç†è§£è¾“å…¥å¹¶æå–å…³é”®ä¿¡æ¯"""
        print(f"ğŸ‘ï¸ [{self.name}] å¼€å§‹æ„ŸçŸ¥é˜¶æ®µ...")
        self.state = AgentState.PERCEIVING

        message = [
            {"role":"system","content":self.system_prompt},
            {"role":"user","content":f"""
            è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·è¾“å…¥ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š
            
            ç”¨æˆ·è¾“å…¥: {input_data}
            
            è¯·æä¾›ä»¥ä¸‹åˆ†æï¼š
            1. ç”¨æˆ·æ„å›¾æ˜¯ä»€ä¹ˆï¼Ÿ
            2. éœ€è¦ä»€ä¹ˆç±»å‹çš„ä»»åŠ¡ï¼Ÿ
            3. æœ‰å“ªäº›å…³é”®å‚æ•°æˆ–æ¡ä»¶ï¼Ÿ
            4. é¢„æœŸçš„è¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿ
            
            è¯·ç”¨JSONæ ¼å¼å›ç­”ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - intent: ç”¨æˆ·æ„å›¾
            - task_type: ä»»åŠ¡ç±»å‹
            - parameters: å…³é”®å‚æ•°
            - expected_output: é¢„æœŸè¾“å‡º
            """}
        ]
        response = self.call_llm(message)

        # å°è¯•è§£æJSONå“åº”
        try:
            # æå–JSONéƒ¨åˆ†
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                print("âœ…æ„ŸçŸ¥é˜¶æ®µæå–åˆ°æœ‰æ•ˆçš„jsonæ ¼å¼")
                json_str = response[json_start:json_end]
                perception_result = json.loads(json_str)
            else:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
        except:
            print("âŒæ„ŸçŸ¥é˜¶æ®µæœªæå–åˆ°æœ‰æ•ˆçš„jsonæ ¼å¼")
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–çš„ç»“æ„
            perception_result = {
                "intent": "ç”¨æˆ·æŸ¥è¯¢",
                "task_type": "ä¿¡æ¯å¤„ç†",
                "parameters": {"query": input_data},
                "expected_output": "ç›¸å…³å›ç­”"
            }
        self.memory.add_short_memory({
            "type":"perceiving",
            "input":input_data,
            "result": perception_result
        })
        print(f"ğŸ“Š æ„ŸçŸ¥ç»“æœ: {json.dumps(perception_result, ensure_ascii=False, indent=2)}")
        return perception_result

    def plan(self, perception_result: dict[str, Any]) -> list[Task]:
        """è§„åˆ’é˜¶æ®µ: åˆ¶å®šæ‰§è¡Œè®¡åˆ’"""
        print(f"ğŸ“‹ [{self.name}] å¼€å§‹è§„åˆ’é˜¶æ®µ...")
        self.state = AgentState.PLANNING
        context = self.memory.get_context()
        message = [
            {"role":"system","content":self.system_prompt},
            {"role": "user", "content": f"""
            åŸºäºä»¥ä¸‹æ„ŸçŸ¥ç»“æœï¼Œè¯·åˆ¶å®šè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’ï¼š
    
            æ„ŸçŸ¥ç»“æœ:
            {json.dumps(perception_result, ensure_ascii=False, indent=2)}
    
            ä¸Šä¸‹æ–‡ä¿¡æ¯:
            {context if context else 'æ— '}
    
            è¯·åˆ¶å®šæ‰§è¡Œè®¡åˆ’ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºå…·ä½“çš„æ­¥éª¤ã€‚
            è¯·ç”¨JSONæ ¼å¼å›ç­”ï¼ŒåŒ…å«ä»¥ä¸‹ç»“æ„ï¼š
            {{
              "tasks": [
                {{
                  "id": "task_1",
                  "description": "ä»»åŠ¡æè¿°",
                  "priority": 1
                }}
              ]
            }}
            """}
        ]
        response = self.call_llm(message)
        # è§£æè§„åˆ’ç»“æœ
        try:
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                plan_result = json.loads(json_str)
                tasks_data = plan_result.get('tasks', [])
                print("âœ…è®¡åˆ’é˜¶æ®µæå–åˆ°æœ‰æ•ˆçš„jsonæ ¼å¼")
            else:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
        except:
            print("âŒè®¡åˆ’é˜¶æ®µæœªæå–åˆ°æœ‰æ•ˆçš„jsonæ ¼å¼")
            # é»˜è®¤ä»»åŠ¡
            tasks_data = [{
                "id": "task_1",
                "description": f"å¤„ç†ç”¨æˆ·è¯·æ±‚: {perception_result.get('intent', 'æœªçŸ¥')}",
                "priority": 1
            }]

        tasks = []
        for task_data in tasks_data:
            task = Task(
                id=task_data['id'],
                description=task_data['description'],
                priority=task_data.get('priority', 1)
            )
            tasks.append(task)
        self.tasks = tasks
        self.memory.add_short_memory({
            "type":"planning",
            "tasks": [task.description for task in tasks]
        })
        print(f"ğŸ“ è§„åˆ’å®Œæˆï¼Œç”Ÿæˆ {len(tasks)} ä¸ªä»»åŠ¡:")
        for i, task in enumerate(tasks, 1):
            print(f"  {i}. {task.description}")

        return tasks

    def execute(self, tasks: list[Task]) -> list[dict[str, Any]]:
        """æ‰§è¡Œé˜¶æ®µ"""
        print(f"âš¡ [{self.name}] å¼€å§‹æ‰§è¡Œé˜¶æ®µ...")
        self.state = AgentState.ACTING

        results = []

        for task in tasks:
            print(f"ğŸ”„ æ‰§è¡Œä»»åŠ¡: {task.description}")
            task.status = "in_progress"
            try:
                context = self.memory.get_context()
                messages = [
                    {"role":"system","content":self.system_prompt},
                    {"role": "user", "content": f"""
                    è¯·æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
                    
                    ä»»åŠ¡æè¿°: {task.description}
                    
                    ä¸Šä¸‹æ–‡ä¿¡æ¯:
                    {context if context else 'æ— '}
                    
                    è¯·æä¾›è¯¦ç»†çš„æ‰§è¡Œç»“æœå’Œè¿‡ç¨‹è¯´æ˜ã€‚
                    """}
                ]
                response = self.call_llm(messages)
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                task.status = "completed"
                task.result = response
                task.completed_at = datetime.now().isoformat()

                result = {
                    "task_id": task.id,
                    "task_description": task.description,
                    "status": "success",
                    "result": response
                }

                print(f"âœ… ä»»åŠ¡å®Œæˆ: {task.description}")
            except Exception as e:
                task.status = "failed"
                task.error = str(e)

                result = {
                    "task_id": task.id,
                    "task_description": task.description,
                    "status": "error",
                    "error": str(e)
                }

                print(f"âŒ ä»»åŠ¡å¤±è´¥: {task.description} - {str(e)}")

            results.append(result)
            time.sleep(0.5) # é¿å…apiè°ƒç”¨è¿‡äºé¢‘ç¹

        self.memory.add_short_memory({
            "type":"execution",
            "result": results
        })
        return results

    def reflect(self, execution_results: list[dict[str, Any]]) -> str:
        """åæ€é˜¶æ®µï¼šåˆ†ææ‰§è¡Œç»“æœå¹¶æ€»ç»“ç»éªŒ"""
        print(f"ğŸ¤” [{self.name}] å¼€å§‹åæ€é˜¶æ®µ...")
        self.state = AgentState.REFLECTING

        # ç»Ÿè®¡æ‰§è¡Œæƒ…å†µ
        total_tasks = len(execution_results)
        successful_tasks = len([r for r in execution_results if r['status'] == 'success'])
        failed_tasks = total_tasks - successful_tasks
        # æ„å»ºåæ€æç¤ºè¯
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": f"""
        è¯·å¯¹ä»¥ä¸‹æ‰§è¡Œç»“æœè¿›è¡Œåæ€å’Œæ€»ç»“ï¼š

        æ‰§è¡Œç»Ÿè®¡:
        - æ€»ä»»åŠ¡æ•°: {total_tasks}
        - æˆåŠŸä»»åŠ¡: {successful_tasks}
        - å¤±è´¥ä»»åŠ¡: {failed_tasks}

        è¯¦ç»†ç»“æœ:
        {json.dumps(execution_results, ensure_ascii=False, indent=2)}

        è¯·æä¾›ï¼š
        1. æ•´ä½“æ‰§è¡Œæ•ˆæœè¯„ä»·
        2. æˆåŠŸå› ç´ åˆ†æ
        3. å¤±è´¥åŸå› åˆ†æï¼ˆå¦‚æœ‰ï¼‰
        4. æ”¹è¿›å»ºè®®
        5. å­¦åˆ°çš„ç»éªŒ
        """}
        ]
        reflection_result = self.call_llm(messages)
        # è®°å½•åæ€ç»“æœ
        self.memory.add_short_memory({
            "type": "reflection",
            "content": reflection_result,
            "stats": {
                "total_tasks": total_tasks,
                "successful_tasks": successful_tasks,
                "failed_tasks": failed_tasks
            }
        })

        # æ›´æ–°é•¿æœŸè®°å¿†
        self.memory.long_memory['last_reflection'] = {
            "timestamp": datetime.now().isoformat(),
            "content": reflection_result,
            "performance": {
                "success_rate": successful_tasks / total_tasks if total_tasks > 0 else 0
            }
        }

        self.state = AgentState.IDLE
        print(f"ğŸ“Š åæ€å®Œæˆï¼ŒæˆåŠŸç‡: {successful_tasks}/{total_tasks}")

        return reflection_result

    def process(self, user_input: str) -> str:
        """å®Œæˆçš„å¤„ç†é€»è¾‘: æ„ŸçŸ¥ã€è§„åˆ’ã€æ‰§è¡Œã€åæ€"""
        print(f"\nğŸš€ [{self.name}] å¼€å§‹å¤„ç†ç”¨æˆ·è¯·æ±‚...")
        print(f"ğŸ“¥ ç”¨æˆ·è¾“å…¥: {user_input}")
        print("=" * 50)
        try:
            # 1. æ„ŸçŸ¥é˜¶æ®µ
            perception_result = self.perceive(user_input)
            print()

            # 2. è§„åˆ’é˜¶æ®µ
            tasks = self.plan(perception_result)
            print()

            # 3. æ‰§è¡Œé˜¶æ®µ
            execution_results = self.execute(tasks)
            print()

            # 4. åæ€é˜¶æ®µ
            reflection = self.reflect(execution_results)
            print()

            # 5. ç”Ÿæˆæœ€ç»ˆå“åº”
            successful_results = [r for r in execution_results if r['status'] == 'success']
            if successful_results:
                final_response = "\n\n".join([r['result'] for r in successful_results])
            else:
                final_response = "æŠ±æ­‰ï¼Œåœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†ä¸€äº›é—®é¢˜ã€‚è¯·æŸ¥çœ‹è¯¦ç»†çš„æ‰§è¡Œæ—¥å¿—ã€‚"

            print("=" * 50)
            print(f"ğŸ“¤ [{self.name}] å¤„ç†å®Œæˆï¼")
            print(f"ğŸ’­ æœ€ç»ˆå›ç­”:\n{final_response}")

            return final_response

        except Exception as e:
            self.state = AgentState.ERROR
            error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
            print(f"âŒ {error_msg}")
            return error_msg

    def get_status(self) -> dict[str, Any]:
        """è·å–Agentå½“å‰çŠ¶æ€"""
        return {
            "name": self.name,
            "role": self.role,
            "state": self.state.value,
            "memory_size": len(self.memory.short_memory),
            "tasks_count": len(self.tasks),
            "completed_tasks": len([t for t in self.tasks if t.status == "completed"]),
            "failed_tasks": len([t for t in self.tasks if t.status == "failed"])
        }
if __name__ == '__main__':
    model = os.getenv("LLM_MODEL_ID")
    apiKey = os.getenv("LLM_API_KEY")
    baseUrl = os.getenv("LLM_BASE_URL")
    messages = [
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
            ]
    llm_client = LlmClient(model= model,apiKey=apiKey, baseUrl=baseUrl)
    #llm_client.chat(messages)
    agent = SimpleAgent(name="å°æ™º", role="Agentå¼€å‘ä¸“å®¶", llm = llm_client)
    # æŸ¥çœ‹Agentåˆå§‹çŠ¶æ€
    print("ğŸ” Agentåˆå§‹çŠ¶æ€:")
    status = agent.get_status()
    for key, value in status.items():
        print(f"  {key}: {value}")

    # æµ‹è¯•ç®€å•é—®ç­”
   # response1 = agent.process(user_input="è¯·è§£é‡Šä¸€ä¸‹ç¥æ¶é­”æ˜¯æœºå™¨å­¦ä¹ ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªç®€å•çš„ä¾‹å­")

    # æµ‹è¯•å¤æ‚ä»»åŠ¡
    response2 = agent.process(
        "æˆ‘æƒ³è¦å¼€å‘ä¸€ä¸ªæ—…æ¸¸åŠ©æ‰‹agentï¼Œè¯·å¸®æˆ‘åˆ¶å®šä¸€ä¸ªæ„å»ºAgentçš„æµç¨‹ï¼Œè¿™é‡Œé¢æˆ‘çš„å¼€å‘è¯­è¨€ä¸ºpythonï¼Œå¹¶ä¸”ä¸æ¶‰åŠæ¨¡å‹ç”Ÿæˆã€‚æˆ‘å°†ä¼šè°ƒç”¨å·²æœ‰çš„æ¨¡å‹æ¥å®ç°ï¼Œé€‰æ‹©çš„Agentæ¶æ„æ¨¡å¼ä¸ºReAct"
    )

    # print("ğŸ§  Agentè®°å¿†çŠ¶å†µ:")
    # print(f"ğŸ“ çŸ­æœŸè®°å¿†æ¡æ•°: {len(agent.memory.short_memory)}")
    # print(f"ğŸ—ƒï¸ é•¿æœŸè®°å¿†: {list(agent.memory.long_memory.keys())}")
    # print(f"ğŸ’­ å·¥ä½œè®°å¿†: {list(agent.memory.working_memory.keys())}")
    #
    # print("\nğŸ“Š æœ€è¿‘çš„è®°å¿†å†…å®¹:")
    # for i, memory in enumerate(agent.memory.short_memory[-3:], 1):
    #     print(f"  {i}. [{memory.get('type', 'unknown')}] {memory.get('timestamp', 'no_time')}")
    #     if memory['type'] == 'reflection':
    #         print(f"     åæ€å†…å®¹: {memory.get('content', '')[:100]}...")
    #
    # # æŸ¥çœ‹ä»»åŠ¡æ‰§è¡Œå†å²
    # print("\nğŸ“‹ ä»»åŠ¡æ‰§è¡Œå†å²:")
    # for i, task in enumerate(agent.tasks, 1):
    #     print(f"  {i}. [{task.status}] {task.description}")
    #     if task.status == "completed":
    #         print(f"     âœ… å®Œæˆæ—¶é—´: {task.completed_at}")
    #     elif task.status == "failed":
    #         print(f"     âŒ é”™è¯¯ä¿¡æ¯: {task.error}")